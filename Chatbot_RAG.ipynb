{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihebakermi10/Chatbot/blob/main/Chatbot_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f972f82d",
      "metadata": {
        "id": "f972f82d"
      },
      "source": [
        "### Basic working of Google Palm LLM in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bwpQtutAgXnx",
      "metadata": {
        "id": "bwpQtutAgXnx"
      },
      "outputs": [],
      "source": [
        "%pip install langchain==0.0.284\n",
        "%pip install python-dotenv==1.0.0\n",
        "%pip install streamlit==1.22.0\n",
        "%pip install tiktoken==0.4.0\n",
        "%pip install faiss-cpu==1.7.4\n",
        "%pip install protobuf~=3.19.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7rFOqtBOqb5e",
      "metadata": {
        "id": "7rFOqtBOqb5e"
      },
      "outputs": [],
      "source": [
        "pip install InstructorEmbedding sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34aa70b",
      "metadata": {
        "id": "a34aa70b"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import GooglePalm\n",
        "\n",
        "api_key = 'AIzaSyD3Vbd0O96sGuRQG2sXF7wfTxAvDf6GaIY'\n",
        "\n",
        "llm = GooglePalm(google_api_key=api_key, temperature=0.1) #creativity  --> 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b610123",
      "metadata": {
        "id": "2b610123"
      },
      "outputs": [],
      "source": [
        "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
        "print(poem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c235a80e",
      "metadata": {
        "id": "c235a80e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "essay = llm(\"write email requesting refund for electronic item\")\n",
        "print(essay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227816a9",
      "metadata": {
        "id": "227816a9"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.llms import GooglePalm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765695b5",
      "metadata": {
        "id": "765695b5"
      },
      "source": [
        "### Now let's load data from Codebasics FAQ csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c62e35c",
      "metadata": {
        "id": "0c62e35c"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path='/content/questions_reponses_digital_rogue_wave.csv', source_column=\"prompt\")\n",
        "\n",
        "# Store the loaded data in the 'data' variable\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd45e51",
      "metadata": {
        "id": "4dd45e51"
      },
      "source": [
        "### Hugging Face Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a4de8c",
      "metadata": {
        "id": "04a4de8c"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "# Initialize instructor embeddings using the Hugging Face model\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\")\n",
        "\n",
        "e = instructor_embeddings.embed_query(\"What is your refund policy?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0762eeac",
      "metadata": {
        "id": "0762eeac"
      },
      "outputs": [],
      "source": [
        "len(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6fab6ce",
      "metadata": {
        "id": "e6fab6ce"
      },
      "outputs": [],
      "source": [
        "e[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e571c0d2",
      "metadata": {
        "id": "e571c0d2"
      },
      "source": [
        "As you can see above, embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\". If you are curious to know about embeddings, go to youtube and search \"codebasics word embeddings\" and you will find bunch of videos with simple, intuitive explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc80a28a",
      "metadata": {
        "id": "cc80a28a"
      },
      "source": [
        "### Vector store using FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c706da",
      "metadata": {
        "id": "b3c706da"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Create a FAISS instance for vector database from 'data'\n",
        "vectordb = FAISS.from_documents(documents=data,\n",
        "                                 embedding=instructor_embeddings)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "retriever = vectordb.as_retriever(score_threshold = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd58f6f",
      "metadata": {
        "id": "cfd58f6f"
      },
      "outputs": [],
      "source": [
        "rdocs = retriever.get_relevant_documents(\"how about job placement support?\")\n",
        "rdocs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf6b257",
      "metadata": {
        "id": "4cf6b257"
      },
      "source": [
        "As you can see above, the retriever that was created using FAISS and hugging face embedding is now capable of pulling relavant documents from our original CSV file knowledge store. This is very powerful and it will help us further in our project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bee857",
      "metadata": {
        "id": "45bee857"
      },
      "source": [
        "##### Embeddings can be created using GooglePalm too. Also for vector database you can use chromadb as well as shown below. During our experimentation, we found hugging face embeddings and FAISS to be more appropriate for our use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a93d079d",
      "metadata": {
        "id": "a93d079d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
        "\n",
        "# from langchain.vectorstores import Chroma\n",
        "# vectordb = Chroma.from_documents(data,\n",
        "#                            embedding=google_palm_embeddings,\n",
        "#                            persist_directory='./chromadb')\n",
        "# vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f3d927",
      "metadata": {
        "id": "02f3d927"
      },
      "source": [
        "### Create RetrievalQA chain along with prompt template üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4842c8",
      "metadata": {
        "id": "2d4842c8"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=retriever,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152a4cf8",
      "metadata": {
        "id": "152a4cf8"
      },
      "source": [
        "### We are all set üëçüèº Let's ask some questions now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90166e8d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "90166e8d"
      },
      "outputs": [],
      "source": [
        "chain('Digital Rogue Wave Maintenance Services?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a4e3e4",
      "metadata": {
        "id": "b3a4e3e4"
      },
      "source": [
        "**As you can see above, the answer of question comes from two different FAQs within our csv file and it is able to pull those questions and merge them nicely**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}